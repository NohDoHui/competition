---
title: "PCA and XGboost in r"
output: html_notebook
---


PCA부분임!!!
노파심에 디테일하게 주석달아놨는데... 잘 이해될거라 믿음 ㅋㅋ
sub_data 에서 나는 시작했는데 호윤이 니는 전체데이터에서 한번 돌려바.. ㅋㅋㅋ 
```{r}
library(data.table)

sub_data <- fread('sub_data.csv') #전체 데이터 불러서 해봐~

pca <- prcomp(sub_data[,1:3300]) #전체 변수에 대해 PCA! label, time, id 등 필요없는 칼럼은 인덱싱 제외하고.. 전체데이터에 하려면 1:3300 이거 변경해야될거임.

options(max.print=10000) #서머리 결과 다 볼 수 있게 셋팅해두는거

summary(pca) #서머리 결과 보면서 'Cumulative Proportion'이 95%되는 순간까지 pc성분 변수로 설정하는거 추천!

pca.rot <- pca$rotation %>% data.frame() #변수들이 각 PC설명하는 proportion을 나타내는 데이터프레임 뽑아두기.

pca.variables <- as.matrix(sub_data[,1:3300]) %*% pca$rotation #실제 변수들의 값과 pc를 설명하는 proportion 곱하기(행렬곱)

pca_data <- cbind(sub_data[,3301:3303], 
                  as.data.frame(pca.variables)) #pca돌릴때 제외시켰던 label, id, time 등 변수 다시 추가하기.

selected_pca_data <-pca_data[,c(1:469)] # 모든 pc들을 사용하는건 비효율적이고, 아까 앞에서 말했던처럼 'Cumulative Proportion'이 95%되는 순간까지의 pc성분에서 데이터 잘라주기.

write.csv(pca_data, file = 'pca_data.csv', row.names = FALSE) #pc로 변수화 시킨 데이터 프레임 저장 ㄱㄱ
```



XGboost 코드!!
XG는 디테일하게 내가 설명안해도 구글에 코드가 너무 쉽게 잘 되있어서 잘 이해될거라 믿음!
```{r}
pca_data <- fread('pca_data.csv') %>% as.data.frame()

pca_data$label <- as.factor(pca_data$label)
is.factor(pca_data$label)



#2.XG boost
library(xgboost)
library(dplyr)

# Convert the Species factor to an integer class starting at 0
b_status = pca_data$label
label = as.integer(pca_data$label)-1
pca_data$label = NULL

# Make traing and testing set
n = nrow(pca_data)
train.index = sample(n, floor(0.75*n))
train.data = as.matrix(pca_data[train.index,])
train.label = label[train.index]
test.data = pca_data[-train.index,] %>% as.data.frame()
test.data = filter(test.data, test.data$time<61) %>% as.matrix() #테스트셋은 60초 이하인 데이터로 만들어주기
test.label = label[-train.index]

# Transform the two data sets into xgb.Matrix
xgb.train = xgb.DMatrix(data=train.data,label=train.label)
xgb.test = xgb.DMatrix(data=test.data,label=test.label)

# Define the parameters for multinomial classification
num_class = length(levels(b_status))
params = list(
  booster="gbtree",
  eta=0.001,
  max_depth=5,
  gamma=3,
  subsample=0.75,
  colsample_bytree=1,
  objective="multi:softprob",
  eval_metric="mlogloss",
  num_class=num_class
)

# Train the XGBoost classifer
xgb.fit=xgb.train(
  params=params,
  data=xgb.train,
  nrounds=10000,
  nthreads=1,
  early_stopping_rounds=10,
  watchlist=list(val1=xgb.train,val2=xgb.test),
  verbose=0
)

# Review the final model and results
xgb.fit

# Predict outcomes with the test data
xgb.pred = predict(xgb.fit,test.data,reshape=T)
xgb.pred = as.data.frame(xgb.pred)
colnames(xgb.pred) = levels(b_status)

# Use the predicted label with the highest probability
xgb.pred$prediction = apply(xgb.pred,1,function(x) colnames(xgb.pred)[which.max(x)])
xgb.pred$label = levels(b_status)[test.label+1]

# Calculate the final accuracy
result = sum(xgb.pred$prediction==xgb.pred$label)/nrow(xgb.pred)
print(paste("Final Accuracy =",sprintf("%1.2f%%", 100*result)))
```

